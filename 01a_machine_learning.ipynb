{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative machine learning - Machine Learning\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. A [first definition](#definition) on the concept of machine learning\n",
    "2. An introduction to a simple problem of [linear regression](#regression)\n",
    "4. A detailed implementation of [simple linear regression](#linear)\n",
    "3. An explanation on [model capacity and overfitting](#capacity)\n",
    "4. Two [exercises](#exercises) to explore regression and classification\n",
    "4. An introduction to the [audio datasets](#audio) that we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id=\"definition\"></a>\n",
    "## Defining machine learning\n",
    "\n",
    "In all natural process, there exists complex relations between sets $\\mathcal{X} \\mapsto \\mathcal{Y}$. This can relate some objects with their names, or a cause to a consequence. In most cases, _we do not know the precise relations_ between these sets, all we have is _observations_ such as pairs $(x,y)$, composed of input data $x \\in \\mathcal{X}$, which have a corresponding expected output $y \\in \\mathcal{Y}$. The overarching goal of machine learning is to approximate such _unknown processes_ as a function $\\mathcal{F}_{\\theta}$, which _transforms_ input data $x$ into output data $y$.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/01_machine_learning_basic.png\" align=\"center\"/>\n",
    "</center>\n",
    "\n",
    "Hence, machine learning aims to understand and model the relationship between some (usually complex and high-dimensional) inputs $\\mathbf{x}\\in\\mathcal{X}\\subset\\mathbb{R}^{\\mathcal{X}}$ and outputs $\\mathbf{y}\\in\\mathcal{Y}\\subset\\mathbb{R}^{\\mathcal{Y}}$, given by a set of data examples $\\mathcal{D}=\\left\\{(x_1,y_1),\\cdots,(x_N,y_N)\\right\\}$. This is achieved by defining a parametric model $f_{\\mathbf{\\theta}}\\in\\mathcal{F}$ inside a family of functions $\\mathcal{F}$, which depends on parameters $\\mathbf{\\theta} \\in \\mathbf{\\Theta}$ and that could approximate the underlying relationship. The _learning_ aspect refers to the adjustment of the parameters $\\mathbf{\\theta}$ in order to obtain the best approximation of the given task\n",
    "$$\n",
    "\\begin{equation}\n",
    "f_{\\mathbf{\\theta}}(\\mathbf{x}) = \\bar{\\mathbf{y}}\\approx \\mathbf{y}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Hence, the major elements that we have to define in any machine learning problems are\n",
    "1. **Dataset** : $\\mathcal{D}=\\left\\{(x_1,y_1),\\cdots,(x_N,y_N)\\right\\}$. This dataset has to be representative of the relation $f:\\mathcal{X} \\mapsto \\mathcal{Y}$ that we are looking to model\n",
    "2. **Model** : Our parametric approximation $\\bar{\\mathbf{y}} = f_{\\mathbf{\\theta}}(\\mathbf{x})$, where the choice of family $f_{\\mathbf{\\theta}}\\in\\mathcal{F}$ is critical\n",
    "3. **Loss** : $\\mathcal{L}\\left( \\bar{\\mathbf{y}}, \\mathbf{y} \\mid f_{\\theta}, \\theta \\right)$ allows to measure the amount of errors made by our model\n",
    "4. **Optimization** : Method to find $\\theta^{*}\\in\\Theta$ so that our model minimizes the loss\n",
    "$$\\theta^{*}= \\underset{\\theta}{\\text{argmin }} \\mathcal{L}\\left( \\bar{\\mathbf{y}}, \\mathbf{y} \\mid f_{\\theta}, \\theta \\right)$$\n",
    "\n",
    "\n",
    "To observe this idea in simple setups, we are going to use the `numpy` library and also initialize the homemade course library `cml` and style for future plotting and exercise. We also set the random generator to a fixed point with `rng = np.random.RandomState(1)`, to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cml.plot import initialize_bokeh\n",
    "from cml.panel import initialize_panel\n",
    "from jupyterthemes.stylefx import set_nb_theme\n",
    "from bokeh.io import show\n",
    "initialize_bokeh()\n",
    "initialize_panel()\n",
    "set_nb_theme(\"onedork\")\n",
    "rng = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regression\"></a>\n",
    "## Simple learning problem\n",
    "\n",
    "Imagine that a certain process somewhere follows the form of a quadratic relationship\n",
    "\n",
    "$$\n",
    " y = a x^{2} + bx + c \n",
    "$$\n",
    "\n",
    "In this case, all the **unknown parameters** are that of a polynomial model, therefore we have $\\theta = \\{a, b, c\\}$. However, this is clearly an ideal (clean) case, whereas in natural observations, there might be some noise in our observations\n",
    "$$\n",
    " y = a x^{2} + bx + c +\\epsilon \\quad \\mbox{with} \\quad \\epsilon \\in [-0.1, 0.1]\n",
    "$$\n",
    "\n",
    "An example of such noisy observations for different parameters is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to our function\n",
    "eps = 0.1\n",
    "a, b, c = 5, 2, 0\n",
    "# Generating the corresponding data\n",
    "x = np.linspace(0, 1, 100)\n",
    "poly = np.poly1d([a, b, c])\n",
    "epsilon = np.random.uniform(-eps, eps, x.shape)\n",
    "y = poly(x) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.plot import center_plot, scatter\n",
    "plot = (center_plot(scatter(x, y, title=\"Simple quadratic problem\", toolbar_location=\"left\")))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our main problem is that this function can follow different types of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[5, -5, 4], [-2, 1, 0], [0.1, 1, 1]]\n",
    "# Generating the x axis\n",
    "x = np.linspace(0, 1, 100)\n",
    "plots = []\n",
    "for p in range(len(params)):\n",
    "    poly = np.poly1d(params[p])\n",
    "    epsilon = np.random.uniform(-eps, eps, x.shape)\n",
    "    y = poly(x) + epsilon\n",
    "    plots.append(scatter(x, y, title=\"Problem \"+(str(p+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import Div\n",
    "from bokeh.layouts import column, row\n",
    "plot = center_plot(column(Div(text = \"Observing different problems\", styles={'font-size': '250%'}), row(*plots)))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life settings, this function can also have different levels of noise, as exemplified in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [3, 0, 1]\n",
    "noise_levels = [0.1, 1.0, 8.0]\n",
    "# Generating the x axis\n",
    "x = np.linspace(0, 1, 100)\n",
    "plots = []\n",
    "for p in range(len(noise_levels)):\n",
    "    poly = np.poly1d(params)\n",
    "    epsilon = np.random.uniform(-noise_levels[p],noise_levels[p],x.shape)\n",
    "    y = poly(x) + epsilon\n",
    "    plots.append(scatter(x, y, title=\"Problem \"+(str(p+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = center_plot(column(Div(text = \"Different amounts of noise\", styles={'font-size': '250%'}), row(*plots)))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we will have some observations of a function, and we would like to optimize a function that gets as close as possible to the real function that generated this data. Here, we plot the real function and also _subsample_ our number of observations (having only a few points to find the corresponding function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data and subsampling\n",
    "x_all = np.linspace(0, 1, 100); x_plot = np.linspace(0, 1, 100)\n",
    "rng.shuffle(x_all); x = np.sort(x_all[:50])\n",
    "poly = np.poly1d([3,0,1])\n",
    "# Adding some external noise\n",
    "epsilon = np.random.uniform(-0.2,0.2,x.shape)\n",
    "y = poly(x)+ epsilon\n",
    "p = scatter(x, y, title=\"Learning problem with groundtruth\")\n",
    "p.line(x_plot, poly(x_plot), line_width=6, line_alpha=0.6, color=\"green\", legend_label=r\"True function\")\n",
    "plot = (center_plot(p))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing our observations (interactive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.tasks import RegressionPolynomial\n",
    "explorer = RegressionPolynomial()\n",
    "explorer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using learning libraries (`scikit-learn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first grip on what machine learning does, we will rely on the `scikit-learn` library. This contains already coded models and learning procedure, that will allow us to _learn_ the parameters of this unknown function.\n",
    "\n",
    "Here we already know that we want to use a `PolynomialFeatures` model to perfom `LinearRegression` and that this polynomial should be of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Our data to fit\n",
    "X = x[:, np.newaxis]\n",
    "# Degree of our polynomial\n",
    "degree = 30;\n",
    "# Create our polynomial model for regression\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit the parameters of this model\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model, we can perform _predictions_ from it, meaning that we can infer the output of the function at values that we did not observe originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference points (not observed)\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "# Predict the values\n",
    "y_plot = model.predict(X_plot)\n",
    "# Compute the error of our model at observed points\n",
    "Y_model_err = np.sqrt(np.mean(np.square(y-model.predict(X))))\n",
    "print(f'Model error : {Y_model_err}')\n",
    "# Plot the result\n",
    "p = scatter(x, y, title=\"Training a scikit-learn model\")\n",
    "p.line(x_plot, poly(x_plot), line_width=6, line_alpha=0.6, color=\"red\", legend_label=r\"Trained model\")\n",
    "plot = (center_plot(p))\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.tasks import RegressionPolynomialSolver\n",
    "explorer = RegressionPolynomialSolver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(x, y, degree):\n",
    "    X = x[:, np.newaxis]\n",
    "    # Create our polynomial model for regression\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    # Fit the parameters of this model\n",
    "    model.fit(X, y)\n",
    "    # Predict the values\n",
    "    x_predict = np.linspace(np.min(x), np.max(x), 200)[:, np.newaxis]\n",
    "    y_model = model.predict(x_predict)\n",
    "    return x_predict[:, 0], y_model #np.array(jnp.zeros(y_model.shape))\n",
    "\n",
    "explorer.solve = solve\n",
    "explorer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear\"></a>\n",
    "# Simple linear regression \n",
    "\n",
    "As discussed previously, regression allows to model the relationships that exist between inputs $\\mathbf{x}\\in\\mathbb{R}^{n}$ and a continuous output $y\\in\\mathbb{R}$. In the case of **linear** regression, we assume that the data that we observe comes from a linear relationship in the input, so that\n",
    "$$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\epsilon$$\n",
    "with $\\epsilon$ exhibiting the *observation noise* (also called *residual error*) that is always present in measurements.\n",
    "\n",
    "To understand how we could learn a model approximating this relationship, we start with the case of *simple* linear regression, where $\\mathbf{x}\\in\\mathbb{R}$. This implies that our model will follow\n",
    "$$\\bar{y} = w_0 + w_1 x$$\n",
    "\n",
    "We will measure the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}\\left( \\bar{\\mathbf{y}},\\theta \\right) = \\sum_{i=1}^{n} \\left| y_{i} - \\bar{y}_{i} \\right|^{2} = \\sum_{i=1}^{n} \\left| y_{i} - (w_{0} + w_{1} x_{i}) \\right|^{2}$$\n",
    "\n",
    "Then our goal is to find the most adequate set of parameters $\\theta = \\{w_{0}, w_{1}\\}$, which are those that minimize the MSE loss defined previously. Therefore, we aim to obtain\n",
    "$$\\theta^{*}= \\underset{\\theta}{\\text{argmin }} \\mathcal{L}\\left( \\bar{\\mathbf{y}}, \\mathbf{y} \\mid f_{\\theta}, \\theta \\right)$$\n",
    "\n",
    "To do so, we will implement the **gradient descent** algorithm discussed in the course.\n",
    "\n",
    "## Manual implementation - `NumPy`\n",
    "\n",
    "We start by performing a *full manual implementation*, in the sense that we need to manually derive the gradient in order to apply the gradient descent updates. To do so, we will rely on [NumPy](https://numpy.org/), which is a fundamental library for numerical computing offering support for N-dimensional arrays and scientific computing tasks, such as linear algebra, statistical analysis, and matrix manipulation. We strongly encourage you to learn NumPy through the set of [tutorials](https://numpy.org/learn/). For the sake of this introductory tutorial, we will provide the explanation for all of the functions that we will use in this first exercise. After that exercise, we will assume for the rest of the course that knowledge of Numpy should be found online. \n",
    "\n",
    "We start by import libraries (`NumPy`) and set a random seed to ensure that the random number generator produces always a reproducible series of random numbers.\n",
    "\n",
    "**Used functions**\n",
    "- `np.random.seed`: sets the seed for the NumPy random number generator. [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a synthetic dataset.\n",
    "\n",
    "For the sake of this exercise, we will generate the data ourselves, so that we know the true values that we are looking for in advance. Hence, we define a linear relationship following\n",
    "$$y = w^{t}_0 + w^{t}_1 x + \\epsilon$$\n",
    "\n",
    "**Used functions**\n",
    "- `np.random.rand`: generates an array of random numbers uniformly distributed over [0, 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
    "- `np.random.randn`: generates an array of random numbers from the standard normal distribution (mean 0, variance 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)\n",
    "- `np.ones`: generates an array of ones with a specified shape. [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.ones.html)\n",
    "- `np.c_`: concatenates arrays along the second axis. [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.c_.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "true_w0 = 2\n",
    "true_w1 = 3\n",
    "n_obs = 100\n",
    "x = [0, 1]\n",
    "# Input to model\n",
    "x = np.linspace(x[0], x[1], n_obs + 1)\n",
    "# Output (target)\n",
    "y = true_w1 * x + true_w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_bar, y_true):\n",
    "    return np.sum((y_true - y_bar) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w0 = 2\n",
    "true_w1 = 3\n",
    "x_min = 0\n",
    "x_max = 1\n",
    "n_obs = 100\n",
    "# done by rudy\n",
    "# modified the eps to get the randomness\n",
    "eps=np.random.randn(x.shape[0])*0.2\n",
    "x = np.linspace(x_min, x_max, n_obs)\n",
    "y = true_w1 * x + true_w0 + eps\n",
    "print (eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_bar):\n",
    "    return np.sum((y - y_bar) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the MSE loss\n",
    "\n",
    "We will measure the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}\\left( \\bar{\\mathbf{y}},\\theta \\right) = \\sum_{i=1}^{n} \\left| y_{i} - \\bar{y}_{i} \\right|^{2} = \\sum_{i=1}^{n} \\left| y_{i} - (w_{0} + w_{1} x_{i}) \\right|^{2}$$\n",
    "\n",
    "This will allow us to evaluate the performances of our model, but is also the basis for the following gradient descent algorithm.\n",
    "\n",
    "**Used functions**\n",
    "- `np.sum`: computes the sum of the provided array (optionally across a provided `axis`). [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_bar):\n",
    "    return np.sum((y - y_bar) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the gradient descent algorithm.\n",
    "\n",
    "As seen in the course Initialize the weight $w_1$ and bias $w_0$ to small random values\n",
    "1. Evaluate the predictions made by our model \n",
    "$$\\hat{y} = w_{1}x + w_0$$\n",
    "2. Compute the mean squared error (MSE) loss between the predicted values and the ground truth labels: \n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\bar{y_i} - y_i)^2$$\n",
    "3. Compute the gradients of the loss with respect to the weight and bias (see derivation in the course)\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\sum_{i} 2 * (\\bar{y}_{i} - y_{i})*x_{i}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_0} = \\sum_{i} 2 * (\\bar{y}_{i} - y_{i})$$\n",
    "4. Update the weights and bias using the gradients and a learning rate $\\eta$: \n",
    "$$w_1 \\leftarrow w_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_1}$$ \n",
    "$$w_0 \\leftarrow w_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "loss = np.zeros(n_iter)\n",
    "eta = 1e-3\n",
    "def model(x, w0, w1):\n",
    "    return x * w1 + w0\n",
    "w0 = np.random.randn()\n",
    "w1 = np.random.randn()\n",
    "for i in range(n_iter):\n",
    "    y_bar = model(x, w0, w1)\n",
    "    l_mse = mse_loss(y, y_bar)\n",
    "    #print(f'w0 = {w0}, w1 = {w1}, loss = {l_mse}')\n",
    "    dldw0 = np.sum(2 * (y_bar - y))\n",
    "    w0 = w0 - eta * dldw0\n",
    "    dldw1 = np.sum(2 * (y_bar - y) * x)\n",
    "    w1 = w1 - eta * dldw1\n",
    "    loss[i] = l_mse\n",
    "import matplotlib.pyplot as plt\n",
    "print(w0)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_iter = 1000\n",
    "lr = 0.001\n",
    "def gradient_descent(x, y, n_iter, lr):\n",
    "    # Initialize the parameters\n",
    "    w_1 = np.random.randn()\n",
    "    w_0 = np.random.randn()\n",
    "    # Perform gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # 1. Calculate the predictions\n",
    "        y_bar = w_1 * x + w_0\n",
    "        # 2. Compute the loss\n",
    "        loss = mse_loss(y, y_bar)\n",
    "        # 3. Calculate the gradients\n",
    "        dw_1 = 2 * np.sum((y_bar - y) * x)\n",
    "        dw_0 = 2 * np.sum((y_bar - y))\n",
    "        # 4. Update the parameters\n",
    "        w_1 -= lr * dw_1\n",
    "        w_0 -= lr * dw_0\n",
    "    return w_1, w_0\n",
    "w_1, w_0 = gradient_descent(x, y, n_iter, lr)\n",
    "print(w_1)\n",
    "print(w_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results.\n",
    "\n",
    "We can see how well our model fits to the observed data by plotting it against the observed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = scatter(x, y, title=\"Simple linear regression\")\n",
    "p.line(x, w_1 * x + w_0, line_width=6, line_alpha=0.6, color=\"red\", legend_label=r\"Learned model\")\n",
    "plot = (center_plot(p))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing our solution interactively\n",
    "\n",
    "In the following code, you can observe the behavior of the model by playing interactively with the properties of the original problem. \n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> Note that each change in the properties of the original problem requires to run the gradient descent algorithm entirely each time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.tasks import RegressionLinearSolver\n",
    "explorer = RegressionLinearSolver()\n",
    "def solve(x, y):\n",
    "    w_1, w_0 = gradient_descent(x, y, n_iter, lr)\n",
    "    x_predict = np.linspace(np.min(x), np.max(x), 200)\n",
    "    y_model = w_1 * x_predict + w_0\n",
    "    return np.array(x_predict), np.array(y_model) #np.array(np.zeros(y_model.shape))\n",
    "\n",
    "explorer.solve = solve\n",
    "explorer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Going further\n",
    "> **Exercise 1 \\[Learning rate\\]**: Experiment with different learning rates (e.g., 0.001, 0.01, 0.1, 1) and observe how they affect the convergence of the gradient descent algorithm. Plot the convergence history (cost function value vs. iteration) for each learning rate.\n",
    "\n",
    "> **Exercise 2 \\[Initialization\\]**: Experiment with different initializations of the coefficients (zeros, random values, etc.) and observe their impact on the convergence of the gradient descent algorithm.\n",
    "\n",
    "> **Exercise 3 \\[Variants\\]**: Implement and compare different variants of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent. Analyze their convergence properties and computational efficiency.\n",
    "\n",
    "> **Exercise 4 \\[Regularization\\]**: Implement Lasso and Ridge regression with gradient descent by incorporating the regularization terms into the cost function and gradient calculations. Compare their performance with the standard linear regression implementation and analyze their impact on the learned coefficients.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discovering automatic differentiation - `JAX`\n",
    "\n",
    "The previous implementation required us to perform manual differentiation of our loss function to understand how to update the parameters. However, large developments have been made in the field of **automatic differentiation**. \n",
    "\n",
    "The recent library [JAX](https://github.com/google/jax) extends NumPy with this automatic differentiation feature (*autograd*), while providing a functional approach to numerical computing, allowing for easy gradient computation and just-in-time (JIT) compilation. Its ability to handle complex and custom gradients makes JAX particularly well-suited for advanced research projects. Similar to NumPy, we strongly encourage you to learn JAX through the set of [tutorials](https://jax.readthedocs.io/en/latest/), but we will also provide here the explanation for all of the functions that we will use in this first exercise. After that, we will assume that knowledge of JAX should be found online. \n",
    "\n",
    "Note that `JAX` has been thought as an extension of `NumPy`, therefore an extremely large portion of its API simply mirrors the `NumPy` functions by adding automatic differentiation features to it.\n",
    "\n",
    "**Used functions**\n",
    "- `random.PRNGKey`: function to generate a key for the pseudorandom number generator. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset.\n",
    "\n",
    "We can simply keep our previous approach to generating the dataset but we will rely on `jnp.ndarray` instead of `np.ndarray`. We also provide the code for splitting the dataset between a `training` and `validation` dataset, as discussed in the course\n",
    "\n",
    "**Used functions**\n",
    "- `random.split`: function to split a PRNGKey into a list of subkeys [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html)\n",
    "- `random.uniform`: function to generate an array of uniformly distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html)\n",
    "- `random.normal`: function to generate an array of normally distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w0 = 2\n",
    "true_w1 = 3\n",
    "key_0, key_1, key = random.split(key, 3)\n",
    "x = random.uniform(key_0, (100,))\n",
    "y = true_w0 + true_w1 * x + (random.normal(key_1, (100,)) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset\n",
    "train_size = int(0.8 * len(x))\n",
    "x_train, x_valid = x[:train_size], x[train_size:]\n",
    "y_train, y_valid = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Similarly Define the cost function and its gradient.\n",
    "\n",
    "**Used functions**\n",
    "- `jnp.mean`: function to compute the mean of an array [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, y, w_1, w_0):\n",
    "    y_bar = x * w_1 + w_0\n",
    "    return jnp.sum((y - y_bar) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(model_loss)(x, y, w_1, w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, y, w_1, w_0):\n",
    "    y_pred = x * w_1 + w_0\n",
    "    residuals = y_pred - y\n",
    "    return jnp.mean(residuals**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Used functions**\n",
    "- `jit`: function to compile a function for faster execution [Documentation](https://jax.readthedocs.io/en/latest/jit.html)\n",
    "- `grad`: function to compute the gradient of a function [Documentation](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_loss_function = jit(grad(loss_function, argnums=[2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the gradient descent algorithm.\n",
    "\n",
    "Note that nowhere in the code do we need to explicitly define the gradients of the different variables. Yet, the optimization will be performed adequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(key, x, y, lr=0.05, n_iter=1000):\n",
    "    m = x.shape\n",
    "    k_0, k_1 = random.split(key, 2)\n",
    "    w_0 = random.normal(k_0, (1,))\n",
    "    w_1 = random.normal(k_1, (1,))\n",
    "    loss_history = []\n",
    "    for _ in range(n_iter):\n",
    "        gradients = grad_loss_function(x, y, w_1, w_0)\n",
    "        #print(gradients[0])\n",
    "        w_1 -= lr * gradients[0]\n",
    "        w_0 -= lr * gradients[1]\n",
    "        loss_history.append(loss_function(x, y, w_1, w_0))\n",
    "    return w_1, w_0, loss_history\n",
    "\n",
    "key_gd, key = random.split(key, 2)\n",
    "w_1, w_0, loss_history = gradient_descent(key_gd, x_train, y_train)\n",
    "print(\"Gradient Descent coefficients:\", w_1, w_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our performances\n",
    "\n",
    "Thanks to our previous split of the dataset, we are now able to evaluate the performances of our model on *unseen data*, which is the overarching goal of building such machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "y_valid_gd = w_1 * x_valid + w_0\n",
    "mse_gd = mean_squared_error(y_valid, y_valid_gd)\n",
    "print(\"MSE for Gradient Descent:\", mse_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results.\n",
    "\n",
    "As previously we can witness our results, and also evaluate our solution with our interactive solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = scatter(np.array(x), np.array(y), title=\"Linear regression\")\n",
    "p.line(np.array(x), np.array(w_1 * x + w_0), line_width=6, line_alpha=0.6, color=\"red\", legend_label=r\"Learned model\")\n",
    "plot = (center_plot(p))\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.tasks import RegressionLinearSolver\n",
    "explorer = RegressionLinearSolver()\n",
    "global key\n",
    "key = random.PRNGKey(23)\n",
    "def solve(x, y):\n",
    "    global key\n",
    "    key_gd, key = random.split(key, 2)\n",
    "    w_1, w_0, loss_history = gradient_descent(key_gd, x, y)\n",
    "    x_predict = np.linspace(np.min(x), np.max(x), 200)\n",
    "    y_model = w_1 * x_predict + w_0\n",
    "    return np.array(x_predict), np.array(y_model) #np.array(jnp.zeros(y_model.shape))\n",
    "\n",
    "explorer.solve = solve\n",
    "explorer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Going further\n",
    "\n",
    "> You are encouraged to experiment in a similar way as outlined for NumPy with different learning rates, maximum number of iterations, and regularization techniques, as well as compare the performance (both accuracy and speed) of the implemented algorithms with `scikit-learn` built-in linear regression functions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"capacity\"></a>\n",
    "## Understanding model capacity and selection\n",
    "\n",
    "\n",
    "In real-life problem, we are aiming to find the parameters of a model, but we do not really know what is the _real_ function underlying this process. So what we can decide to select _any_ function of _any_ **capacity** (complexity of the function). One of the problem with that, is that if we have a too simple function, it will _underfit_ (it is not complex enough for our observations). On the opposite end, if we have a function which is too complex, it might be able to _fit through all training points exactly_ ... even though there is noise in our observations ! This is examplified in the following\n",
    "\n",
    "<img src=\"images/01_soa_function_families.png\" align=\"center\"/>\n",
    "\n",
    "We can observe this idea and play with it directly by trying to find a function approximating our previous observations with a polynomial function chosen to have a degree inside \\([1,2,8]\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Depending on the _capacity_ of the model, what we can observe is that\n",
    "\n",
    "- `capacity too low   -> underfitting   : prediction variance >  noise variance`\n",
    "- `adequate capacity  -> good fit       : prediction variance == noise variance`\n",
    "- `capacity too high  -> overfitting    : prediction variance <  noise variance`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar example can be given for a classification problem in two dimensions as follows\n",
    "\n",
    "<img src=\"images/01_underfit.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In the following, we define the exercises that you should fill for this session. These go further than what we have seen together in the course, but they are based on the exact same principles, simply with slightly more complex definitions. We provide an overall guideline for successfully implementing each of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> In this exercise, we will implement linear classification using NumPy and JAX. The goal of this exercise is to gain a better understanding of how linear classification works and how to implement it ourselves using our two first libraries of choice, namely NumPy and JAX. To complete this exercise, you will need to have a basic understanding of NumPy and JAX. You can use the resources provided in the previous exercises to learn more about these libraries.\n",
    "\n",
    "We help you out by first defining a simple linear classification problem to solve\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "# Properties of the problem\n",
    "n_observations = 200\n",
    "noise = 0.2\n",
    "c1_center = [-2, -1]\n",
    "c2_center = [2, 1]\n",
    "# Create points\n",
    "x_coords, y_class = make_blobs(n_samples=n_observations, centers=[c1_center, c2_center], n_features=2, cluster_std=0.55)\n",
    "x_data = x_coords + (noise * np.random.randn(n_observations, 2))\n",
    "#x_data = x_coords\n",
    "y_classes = y_class\n",
    "y_classes[y_classes == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.1 - Implement linear classification with NumPy\n",
    "\n",
    "> 1. Check the training data with two classes that are linearly separable.\n",
    "> 2. Initialize weights and bias.\n",
    "> 3. Implement the forward pass of the linear classifier.\n",
    "> 4. Implement the hinge loss for classification.\n",
    "> 5. Derive the gradients (backward pass) of the linear classifier.\n",
    "> 6. Implement gradient descent to optimize the weights and bias.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Going further (optional)\n",
    "> Try to replace the Hinge loss by the cross-entropy loss in your implementation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.2 - Implement linear classification with JAX\n",
    "\n",
    "> 1. Initialize weights and bias.\n",
    "> 2. Define the loss function using the jax.nn.softmax_cross_entropy_with_logits function.\n",
    "> 3. Implement gradient descent to optimize the weights and bias.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.3 - Compare the results of the NumPy and JAX implementations.\n",
    "> 1. Plot the decision boundary for each implementation.\n",
    "> 2. Compare the training time and accuracy of each implementation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Going further (optional)\n",
    "> Implement a more complex dataset and compare the results of the NumPy and JAX implementations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> In this exercise, you will implement polynomial regression using NumPy and JAX. This should be a quite straightforward extension of what we have seen until now. We deliberately removed details of the implementation (list of points to adress) so that you can start defining your first entire machine learning setup by yourself.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.1 - Implement polynomial regression using NumPy with gradient descent.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.2. Implement polynomial regression using JAX with gradient descent.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.3 - Compare the results of both implementations and the speed of the optimization algorithm.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"audio\"></a>\n",
    "# Audio applications\n",
    "\n",
    "In order to test our algorithms on audio and music data, we will work with several datasets. We will both be using well-known state-of-art datasets (through the TF dataset system). But first, we will rely on a simple dataset (`Musclefish`) that should be downloaded on your local computer first from this [link](https://nubo.ircam.fr/index.php/s/ByK4QL7nE4Mq5MA)\n",
    "\n",
    "**The following set of instructions will perform that automatically for you :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://nubo.ircam.fr/index.php/s/ByK4QL7nE4Mq5MA/download/musclefish.zip --output musclefish.zip\n",
    "!unzip musclefish.zip\n",
    "!mkdir data\n",
    "!mv musclefish data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset details\n",
    "\n",
    "  |**Type**|*Origin*|\n",
    "  |-------:|:---------|\n",
    "  |**Classification**|[*MuscleFish*](http://knight.cis.temple.edu/~vasilis/Courses/CIS750/Papers/muscle_fish.pdf) dataset|\n",
    "  |**Music-speech**|[*MIREX Recognition*](http://www.music-ir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection) set|\n",
    "  |**Source separation**|[*SMC Mirum*](http://smc.inesctec.pt/research/data-2/) dataset|\n",
    "  |**Speech recognition**|[*CMU Arctic*](http://festvox.org/cmu_arctic/) dataset|\n",
    "\n",
    "**Unzip the file and place the `musclefish` folder inside a `data` folder in the root of this notebook**\n",
    "For the first parts of the tutorial, we will mostly rely solely on the classification dataset. In order to facilitate the interactions, we provide a dataset class called `AudioSupervisedDataset` that will allow to import all audio datasets along the tutorials. This class also contains a set of holders and will allow us to perform batch-wise gradient descent.\n",
    "\n",
    "```Python\n",
    "class AudioSupervisedDataset():\n",
    "    \"\"\"\n",
    "    Helper class to import datasets\n",
    "    % class_path  : Path to the dataset (string)\n",
    "    % type       : Type of dataset (string: 'classify', 'plain', 'metadata')\n",
    "    \"\"\" \n",
    "    audio_files\n",
    "    labels\n",
    "    labels_names\n",
    "    num_examples \n",
    "    num_classes \n",
    "```\n",
    "\n",
    "We also provide a simplified function `import_dataset` that is demonstrated below.\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "**_Exercise_**  \n",
    "\n",
    "  1. Launch the import procedure and check the corresponding structure\n",
    "  2. Code a count function that prints the name and number of examples for each classes \n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cml.data import import_dataset, AudioSupervisedDataset\n",
    "data_path = \"data/\"\n",
    "batch_size = 16\n",
    "# Instantiate the dataset class\n",
    "train_dataset = import_dataset(data_path, \"musclefish\", 'train', batch_size=batch_size)\n",
    "# Generate a batch of data from the train dataset\n",
    "batch_x, batch_y = next(train_dataset)\n",
    "# Print the shape of the input and output batch tensors\n",
    "print(batch_x.shape)  # (16, 44100)\n",
    "print(batch_y.shape)  # (16,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Q-0.1.2 - Count function to print the number of examples in each class along with class label\n",
    "\n",
    "n_batch = 0\n",
    "train_dataset._reset_generator()\n",
    "for batch_x, batch_y in iter(train_dataset):\n",
    "    n_batch += 1\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "\n",
    "print(n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We will rely on a set of spectral transforms that allow to obtain a more descriptive view over the audio information. As most of these are out of the scope of the machine learning course, we redirect you to a [signal processing course](https://ccrma.stanford.edu/~jos/sasp/) proposed by [Julius O. Smith](https://ccrma.stanford.edu/~jos/).  \n",
    "\n",
    "The following functions to compute various types of transforms are given as part of the basic audio dataset class, in the `cml.data.audio` package\n",
    "\n",
    "  |**Name**|*Transform*|\n",
    "  |-------:|:----------|\n",
    "  |`stft`       |[Short-term Fourier transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform)|\n",
    "  |`mel`  |[Mel scale](https://en.wikipedia.org/wiki/Mel_scale) transform|\n",
    "  |`chroma` |[Chromas vector](https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles)|\n",
    "  |`cqt`        |[Constant-Q](https://en.wikipedia.org/wiki/Constant_Q_transform) transform|\n",
    "\n",
    "In order to perform the various computations, we provide the following function, in the `AudioSupervisedDataset` which performs the different transforms on a complete dataset.  \n",
    "\n",
    "``` Python\n",
    "dataset.transform(index, name)\n",
    "    \"\"\" index   : Specific index in our dataset \"\"\"\n",
    "    \"\"\" name    : Name of the transform to apply \"\"\"\n",
    "\n",
    "# The name can be selected in \n",
    "\"stft\"      # Power spectrum (STFT)\n",
    "\"mel\"       # Spectrum in Mel scale\n",
    "\"chroma\"    # Chroma vectors\n",
    "\"cqt\"       # Constant-Q transform\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Launch the transform computation procedure and check the corresponding structure\n",
    "  2. For each class, select a random element and plot its various transforms on a single plot. You should obtain plots similar to those shown afterwards.\n",
    "  3. For each transform, try to spot major pros and cons of their representation.\n",
    "  \n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 - Pre-process the audio to obtain spectral transforms \n",
    "power_spec = train_dataset.transform(0, \"stft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Q-0.2.2 - Plot the various transforms \n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "<div markdown = \"1\">\n",
    "\n",
    "As you might have noted from the previous exercice, most spectral transforms have a very high dimensionality, and might not be suited to exhibit the relevant structure of different classes. To that end, we provide a set of functions for computing several spectral features in the `cml.data` package, we redirect interested readers to this [exhaustive article](http://recherche.ircam.fr/anasyn/peeters/ARTICLES/Peeters_2003_cuidadoaudiofeatures.pdf) on spectral features computation.\n",
    "\n",
    "  |**File**|*Transform*|\n",
    "  |-------:|:----------|\n",
    "  |`spectral_centroid`|Spectral centroid|\n",
    "  |`spectral_bandwidth`|Spectral bandwidth|\n",
    "  |`spectral_contrast`|Spectral contrast|\n",
    "  |`spectral_flatness`|Spectral flatness|\n",
    "  |`spectral_rolloff`|Spectral rolloff|\n",
    "\n",
    "Once again, we provide a function to perform the computation of different features on a complete set. Note that for each feature, we obtain the temporal evolution in a vector. Therefore, for further learning tasks, if you wish to obtain simplified spaces, you might need to compute the mean and standard deviation of each feature.\n",
    "\n",
    "``` Python\n",
    "dataset.feature(index, name)\n",
    "    \"\"\" index   : Specific index in our dataset \"\"\"\n",
    "    \"\"\" name    : Name of the feature to obtain \"\"\"\n",
    "\n",
    "# Name can be chosen inside the following\n",
    "\"loudness\"    # Loudness\n",
    "\"centroid\"    # Spectral centroid\n",
    "\"bandwidth\"   # Spectral bandwidth\n",
    "\"contrast\"    # Spectral contrast\n",
    "\"flatness\"    # Spectral flatness\n",
    "\"rolloff\"     # Spectral rolloff\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "  1. Launch the feature computation procedure and check the corresponding structure\n",
    "  2. This time for each class, superimpose the plots of various features on a single plot, along with a boxplot of mean and standard deviations. You should obtain plots similar to those shown afterwards.\n",
    "  3. What conclusions can you make on the discriminative power of each feature ?\n",
    "  4. Perform scatter plots of the mean features for all the dataset, while coloring different classes.\n",
    "  5. What conclusions can you make on the discriminative power of mean features ?\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3 - Compute temporal spectral features\n",
    "power_spec = train_dataset.feature(0, \"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Q-0.3.2 - Plot the various features \n",
    "\n",
    "# Use these styles for boxplot\n",
    "boxprops=dict(linewidth=3, color='white')\n",
    "whiskerprops=dict(linewidth=3, color='white')\n",
    "medianprops=dict(linewidth=2.5, color='firebrick')\n",
    "flierprops = dict(markeredgecolor='white', markerfacecolor='firebrick')\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Q-0.3.4 - Observe the distribution of classes for different features\n",
    "\n",
    "# This allows to use 3D rendering in matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.figure(figsize=(12,8))\n",
    "# Create a vector of random colors for each class\n",
    "colorVect = np.zeros((3, len(data_struct[\"class_names\"])));\n",
    "for c in range(len(data_struct[\"class_names\"])):\n",
    "    colorVect[:,c] = np.random.rand(3);\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this tutorial, now remember that we can use any form of description (features) as a basis for learning algorithms. We will see in the next tutorial what we an do with these features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
